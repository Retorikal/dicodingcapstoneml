{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 11:30:53.961151: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-02 11:30:53.961191: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import itertools\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "\n",
    "MAX_HISTORY = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load both dataset and convert to tfds object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slices(features):\n",
    "  for i in itertools.count():\n",
    "    # For each feature take index `i`\n",
    "    example = {name:values[i] for name, values in features.items()}\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load courses list\n",
    "TODO: Course list disini sepertinya rada galengkap, token dari course\" bangkit jadi nol semua\n",
    "\n",
    "Output berupa objek MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_name        : Belajar Fundamental Aplikasi Android\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = pd.read_csv(\"courses.csv\")\n",
    "unique_course_names = courses[\"name\"].tolist()\n",
    "\n",
    "courses_dict = {\n",
    "  \"course_name\": np.array(unique_course_names)\n",
    "}\n",
    "\n",
    "for example in slices(courses_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "\n",
    "tfds_courses = tf.data.Dataset.from_tensor_slices(courses_dict)\n",
    "tfdsmap_courses = tfds_courses.map(lambda x: x['course_name'])\n",
    "tfdsmap_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken_courses      : ['Bangkit General Assessment', 'Belajar Dasar Pemrograman Web', 'Belajar Fundamental Aplikasi Android']\n",
      "recommendation     : Bangkit Android Assessment\n"
     ]
    }
   ],
   "source": [
    "# Load and clean data\n",
    "useract = pd.read_csv(\"user_activities.csv\")\n",
    "useract = useract.drop(columns=['id'])\n",
    "useract = useract.dropna(how='all')\n",
    "\n",
    "def mergeall(rowdata):\n",
    "    aggregated_datas = \",\".join([str(s) for s in list(rowdata.values) if not pd.isna(s)])\n",
    "    return aggregated_datas.split(\",\")\n",
    "\n",
    "def mergecourse(rowdata):\n",
    "    aggregated_datas = \",\".join([str(s) for s in [rowdata.graduated_courses, rowdata.on_progress_courses] if not pd.isna(s)])\n",
    "    return aggregated_datas.split(\",\")\n",
    "\n",
    "merged_datas = list(useract.apply(mergeall, axis = 1))\n",
    "merged_courses = list(useract.apply(mergecourse, axis = 1))\n",
    "\n",
    "agumented_datas_dict = {\n",
    "  'taken_courses': [],\n",
    "  'recommendation': []\n",
    "}\n",
    "\n",
    "# for each merged course, \n",
    "#   for each course in merged course,\n",
    "#     pick 1 as output, lainya jadi input\n",
    "#     push as new entry\n",
    "# TODO: \n",
    "#   This split is not too well-defined on merged courses with length longer than MAX_HISTORY.\n",
    "#   This is because after permutating possible input, trailing courses are cut off at token padding anyway.\n",
    "\n",
    "for i in range(len(merged_datas)):\n",
    "  merged_data = merged_datas[i]\n",
    "  merged_course = merged_courses[i]\n",
    "    \n",
    "  for course in merged_course:\n",
    "    #Simpen dulu buat nanti kalo mau ditambahin ingfo non-courses\n",
    "    #agumented_datas_dict['x'].append([data for data in merged_data if data != course])\n",
    "    agumented_datas_dict['taken_courses'].append([data for data in merged_course if data != course])\n",
    "    agumented_datas_dict['recommendation'].append(course)\n",
    "\n",
    "for example in slices(agumented_datas_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load user activities list\n",
    "User Activities merujuk pada semua course yang pernah dan sedang diambil. Asumsinya adalah kalau banyak user yang ambil bebarengan, maka seharusnya course\" tsb berkaitan.\n",
    "\n",
    "Augmentasi data dilakukan sbb:\n",
    "```\n",
    "Jika course yang pernah diambil adalah [a, b, c, d, e]\n",
    "output akan diaugmentasi menjadi:\n",
    "x             | y\n",
    "--------------+----\n",
    "[a, b, c, d]  | e\n",
    "[a, b, c, e]  | d\n",
    "[a, b, d, e]  | c\n",
    "[a, c, d, e]  | b\n",
    "[b, c, d, e]  | a\n",
    "\n",
    "```\n",
    "\n",
    "Output berupa objek MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken_courses      : [0 0 0 9 1]\n",
      "recommendation     : Bangkit Android Assessment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec={'taken_courses': TensorSpec(shape=(5,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize manual karena usecasenya agak aneh\n",
    "\n",
    "def get_tokenizer_dict(train_list):\n",
    "  return dict(zip(train_list, [i+1 for i in range(len(train_list))]))\n",
    "\n",
    "def pad_tokens(input_sequences, maxlen):\n",
    "  return np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n",
    "\n",
    "def tokenize(tokenizer_dict, corpus, maxlen = 5):\n",
    "  output = []\n",
    "  for line in corpus:\n",
    "    tokenizedline = [tokenizer_dict.get(entry, 0) for entry in line]\n",
    "    output.append(tokenizedline)\n",
    "    \n",
    "  return pad_tokens(output, maxlen)\n",
    "\n",
    "\n",
    "tokenizer_dict = get_tokenizer_dict(unique_course_names)\n",
    "tokenized_datas = tokenize(tokenizer_dict, agumented_datas_dict[\"taken_courses\"], MAX_HISTORY)\n",
    "tokenized_datas_dict = {\n",
    "  'taken_courses': tokenized_datas,\n",
    "  'recommendation': agumented_datas_dict[\"recommendation\"]\n",
    "}\n",
    "\n",
    "for example in slices(tokenized_datas_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "\n",
    "tfds_tokenized_data = tf.data.Dataset.from_tensor_slices(tokenized_datas_dict)\n",
    "tfdsmap_tokenized_data = tfds_tokenized_data.map(lambda x: {\n",
    "  'taken_courses': x['taken_courses'],\n",
    "  'recommendation': x['recommendation']\n",
    "})\n",
    "tfdsmap_tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "1719\n"
     ]
    }
   ],
   "source": [
    "print(len(tfdsmap_courses))\n",
    "print(len(tfdsmap_tokenized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec={'taken_courses': TensorSpec(shape=(5,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}> 1547\n",
      "<TakeDataset element_spec={'taken_courses': TensorSpec(shape=(5,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}> 172\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(69420)\n",
    "shuffled = tfdsmap_tokenized_data.shuffle(len(tfdsmap_tokenized_data), seed=69420, reshuffle_each_iteration=False)\n",
    "\n",
    "train_len = math.floor(len(tfdsmap_tokenized_data) * 0.9)\n",
    "test_len = len(tfdsmap_tokenized_data) - train_len\n",
    "\n",
    "train = shuffled.take(train_len)\n",
    "test = shuffled.skip(train_len).take(test_len)\n",
    "\n",
    "print(train, len(train))\n",
    "print(test, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32\n",
    "\n",
    "#https://www.tensorflow.org/recommenders/examples/multitask/\n",
    "\n",
    "user_properties_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(len(unique_course_names)+1, 32, input_length=5),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(embedding_dimension, activation='relu')\n",
    "])\n",
    "\n",
    "course_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(vocabulary=unique_course_names, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_course_names) + 1, embedding_dimension),\n",
    "  # tf.keras.layers.GRU(embedding_dimension)\n",
    "])\n",
    "\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=tfrs.metrics.FactorizedTopK(\n",
    "    candidates=tfdsmap_courses.batch(16).map(course_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.Model):\n",
    "  def __init__(self, user_model, course_model):\n",
    "    super().__init__()\n",
    "    self.course_model: tf.keras.Model = course_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[tf.Tensor, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"taken_courses\"])\n",
    "\n",
    "    # And pick out the course features and pass them into the course model,\n",
    "    # getting embeddings back.\n",
    "    positive_course_embeddings = self.course_model(features[\"recommendation\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_course_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_15 (Embedding)    (None, 5, 32)             1568      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               16100     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                3232      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,900\n",
      "Trainable params: 20,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " string_lookup_8 (StringLook  (None,)                  0         \n",
      " up)                                                             \n",
      "                                                                 \n",
      " embedding_16 (Embedding)    (None, 32)                1568      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,568\n",
      "Trainable params: 1,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(user_properties_model, course_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "cached_train = train.shuffle(len(tfdsmap_tokenized_data)).batch(400).cache()\n",
    "cached_test = test.batch(40).cache()\n",
    "\n",
    "user_properties_model.summary()\n",
    "course_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 0s 72ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1855 - factorized_top_k/top_5_categorical_accuracy: 0.7460 - factorized_top_k/top_10_categorical_accuracy: 0.8933 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1852.7407 - regularization_loss: 0.0000e+00 - total_loss: 1852.7407\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2030 - factorized_top_k/top_5_categorical_accuracy: 0.7531 - factorized_top_k/top_10_categorical_accuracy: 0.8992 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1835.0516 - regularization_loss: 0.0000e+00 - total_loss: 1835.0516\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 69ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1926 - factorized_top_k/top_5_categorical_accuracy: 0.7518 - factorized_top_k/top_10_categorical_accuracy: 0.8940 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1825.8277 - regularization_loss: 0.0000e+00 - total_loss: 1825.8277\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 74ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1745 - factorized_top_k/top_5_categorical_accuracy: 0.7582 - factorized_top_k/top_10_categorical_accuracy: 0.9017 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1813.9271 - regularization_loss: 0.0000e+00 - total_loss: 1813.9271\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 72ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2165 - factorized_top_k/top_5_categorical_accuracy: 0.7705 - factorized_top_k/top_10_categorical_accuracy: 0.9069 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1794.4450 - regularization_loss: 0.0000e+00 - total_loss: 1794.4450\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 70ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2553 - factorized_top_k/top_5_categorical_accuracy: 0.7893 - factorized_top_k/top_10_categorical_accuracy: 0.9211 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1773.6086 - regularization_loss: 0.0000e+00 - total_loss: 1773.6086\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2301 - factorized_top_k/top_5_categorical_accuracy: 0.8132 - factorized_top_k/top_10_categorical_accuracy: 0.9218 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1763.2292 - regularization_loss: 0.0000e+00 - total_loss: 1763.2292\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 68ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2637 - factorized_top_k/top_5_categorical_accuracy: 0.8100 - factorized_top_k/top_10_categorical_accuracy: 0.9250 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1753.8376 - regularization_loss: 0.0000e+00 - total_loss: 1753.8376\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 73ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2301 - factorized_top_k/top_5_categorical_accuracy: 0.7757 - factorized_top_k/top_10_categorical_accuracy: 0.9237 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1749.1548 - regularization_loss: 0.0000e+00 - total_loss: 1749.1548\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 87ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2579 - factorized_top_k/top_5_categorical_accuracy: 0.7990 - factorized_top_k/top_10_categorical_accuracy: 0.9231 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1742.3316 - regularization_loss: 0.0000e+00 - total_loss: 1742.3316\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 72ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2527 - factorized_top_k/top_5_categorical_accuracy: 0.8041 - factorized_top_k/top_10_categorical_accuracy: 0.9295 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1735.2350 - regularization_loss: 0.0000e+00 - total_loss: 1735.2350\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 71ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2405 - factorized_top_k/top_5_categorical_accuracy: 0.7990 - factorized_top_k/top_10_categorical_accuracy: 0.9224 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1732.1710 - regularization_loss: 0.0000e+00 - total_loss: 1732.1710\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 76ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2637 - factorized_top_k/top_5_categorical_accuracy: 0.7931 - factorized_top_k/top_10_categorical_accuracy: 0.9263 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1727.9144 - regularization_loss: 0.0000e+00 - total_loss: 1727.9144\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2644 - factorized_top_k/top_5_categorical_accuracy: 0.7809 - factorized_top_k/top_10_categorical_accuracy: 0.9347 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1724.4907 - regularization_loss: 0.0000e+00 - total_loss: 1724.4907\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2696 - factorized_top_k/top_5_categorical_accuracy: 0.7886 - factorized_top_k/top_10_categorical_accuracy: 0.9341 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1717.9821 - regularization_loss: 0.0000e+00 - total_loss: 1717.9821\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 71ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2980 - factorized_top_k/top_5_categorical_accuracy: 0.8171 - factorized_top_k/top_10_categorical_accuracy: 0.9347 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1714.2840 - regularization_loss: 0.0000e+00 - total_loss: 1714.2840\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 76ms/step - factorized_top_k/top_1_categorical_accuracy: 0.3090 - factorized_top_k/top_5_categorical_accuracy: 0.7919 - factorized_top_k/top_10_categorical_accuracy: 0.9321 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1718.3307 - regularization_loss: 0.0000e+00 - total_loss: 1718.3307\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 71ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2689 - factorized_top_k/top_5_categorical_accuracy: 0.8151 - factorized_top_k/top_10_categorical_accuracy: 0.9334 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1709.5792 - regularization_loss: 0.0000e+00 - total_loss: 1709.5792\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 71ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2902 - factorized_top_k/top_5_categorical_accuracy: 0.8203 - factorized_top_k/top_10_categorical_accuracy: 0.9347 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1709.4010 - regularization_loss: 0.0000e+00 - total_loss: 1709.4010\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 72ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2683 - factorized_top_k/top_5_categorical_accuracy: 0.8100 - factorized_top_k/top_10_categorical_accuracy: 0.9392 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1706.6932 - regularization_loss: 0.0000e+00 - total_loss: 1706.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fab104b7460>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 64ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1744 - factorized_top_k/top_5_categorical_accuracy: 0.4302 - factorized_top_k/top_10_categorical_accuracy: 0.5930 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 159.6651 - regularization_loss: 0.0000e+00 - total_loss: 159.6651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.1744185984134674,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.43023255467414856,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.5930232405662537,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 1.0,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 1.0,\n",
       " 'loss': 61.37709426879883,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 61.37709426879883}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7fab11641700>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "\n",
    "# recommends courses out of the entire courses dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((\n",
    "    tfdsmap_courses.batch(10), \n",
    "    tfdsmap_courses.batch(10).map(model.course_model)\n",
    "  ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 38 30 15  3]]\n",
      "Recommendation:\n",
      "Memulai Pemrograman Dengan Java\n",
      "Menjadi Front-End Web Developer Expert\n",
      "Belajar Fundamental Front-End Web Development\n",
      "Memulai Pemrograman Dengan Kotlin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get recommendations.\n",
    "taken_courses = [\n",
    "  \"Belajar Dasar Pemrograman Web,Meniti Karier sebagai Software Developer\",\n",
    "  \"Belajar Membuat Augmented Reality dengan Lens Studio\",\n",
    "  \"Belajar Dasar Pemrograman JavaScript\",\n",
    "  \"Belajar Fundamental Front-End Web Development\",\n",
    "  \"Belajar Membuat Aplikasi Android untuk Pemula\"\n",
    "]\n",
    "inputdata = tokenize(tokenizer_dict, [taken_courses], MAX_HISTORY)\n",
    "print(inputdata)\n",
    "\n",
    "_, course_names = index(inputdata)\n",
    "print(\"Recommendation:\")\n",
    "for course_name in course_names[0, :4]:\n",
    "  tf.print(course_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
