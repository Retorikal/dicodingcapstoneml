{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 11:30:53.961151: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-02 11:30:53.961191: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import itertools\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "\n",
    "MAX_HISTORY = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load both dataset and convert to tfds object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slices(features):\n",
    "  for i in itertools.count():\n",
    "    # For each feature take index `i`\n",
    "    example = {name:values[i] for name, values in features.items()}\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_name        : Belajar Fundamental Aplikasi Android\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = pd.read_csv(\"courses.csv\")\n",
    "unique_course_names = courses[\"name\"].tolist()\n",
    "\n",
    "courses_dict = {\n",
    "  \"course_name\": np.array(unique_course_names)\n",
    "}\n",
    "\n",
    "for example in slices(courses_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "\n",
    "tfds_courses = tf.data.Dataset.from_tensor_slices(courses_dict)\n",
    "tfdsmap_courses = tfds_courses.map(lambda x: x['course_name'])\n",
    "tfdsmap_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken_courses      : ['Bangkit General Assessment', 'Belajar Dasar Pemrograman Web', 'Belajar Fundamental Aplikasi Android']\n",
      "recommendation     : Bangkit Android Assessment\n"
     ]
    }
   ],
   "source": [
    "# Load and clean data\n",
    "useract = pd.read_csv(\"user_activities.csv\")\n",
    "useract = useract.drop(columns=['id'])\n",
    "useract = useract.dropna(how='all')\n",
    "\n",
    "def mergeall(rowdata):\n",
    "    aggregated_datas = \",\".join([str(s) for s in list(rowdata.values) if not pd.isna(s)])\n",
    "    return aggregated_datas.split(\",\")\n",
    "\n",
    "def mergecourse(rowdata):\n",
    "    aggregated_datas = \",\".join([str(s) for s in [rowdata.graduated_courses, rowdata.on_progress_courses] if not pd.isna(s)])\n",
    "    return aggregated_datas.split(\",\")\n",
    "\n",
    "merged_datas = list(useract.apply(mergeall, axis = 1))\n",
    "merged_courses = list(useract.apply(mergecourse, axis = 1))\n",
    "\n",
    "agumented_datas_dict = {\n",
    "  'taken_courses': [],\n",
    "  'recommendation': []\n",
    "}\n",
    "\n",
    "# for each merged course, \n",
    "#   for each course in merged course,\n",
    "#     pick 1 as output, lainya jadi input\n",
    "#     push as new entry\n",
    "# TODO: \n",
    "#   This split is not too well-defined on merged courses with length longer than MAX_HISTORY.\n",
    "#   This is because after permutating possible input, trailing courses are cut off at token padding anyway.\n",
    "\n",
    "for i in range(len(merged_datas)):\n",
    "  merged_data = merged_datas[i]\n",
    "  merged_course = merged_courses[i]\n",
    "    \n",
    "  for course in merged_course:\n",
    "    #Simpen dulu buat nanti kalo mau ditambahin ingfo non-courses\n",
    "    #agumented_datas_dict['x'].append([data for data in merged_data if data != course])\n",
    "    agumented_datas_dict['taken_courses'].append([data for data in merged_course if data != course])\n",
    "    agumented_datas_dict['recommendation'].append(course)\n",
    "\n",
    "for example in slices(agumented_datas_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken_courses      : [0 0 0 9 1]\n",
      "recommendation     : Bangkit Android Assessment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec={'taken_courses': TensorSpec(shape=(5,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize manual karena usecasenya agak aneh\n",
    "\n",
    "def get_tokenizer_dict(train_list):\n",
    "  return dict(zip(train_list, [i+1 for i in range(len(train_list))]))\n",
    "\n",
    "def pad_tokens(input_sequences, maxlen):\n",
    "  return np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n",
    "\n",
    "def tokenize(tokenizer_dict, corpus, maxlen = 5):\n",
    "  output = []\n",
    "  for line in corpus:\n",
    "    tokenizedline = [tokenizer_dict.get(entry, 0) for entry in line]\n",
    "    output.append(tokenizedline)\n",
    "    \n",
    "  return pad_tokens(output, maxlen)\n",
    "\n",
    "\n",
    "tokenizer_dict = get_tokenizer_dict(unique_course_names)\n",
    "tokenized_datas = tokenize(tokenizer_dict, agumented_datas_dict[\"taken_courses\"], MAX_HISTORY)\n",
    "tokenized_datas_dict = {\n",
    "  'taken_courses': tokenized_datas,\n",
    "  'recommendation': agumented_datas_dict[\"recommendation\"]\n",
    "}\n",
    "\n",
    "for example in slices(tokenized_datas_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "\n",
    "tfds_tokenized_data = tf.data.Dataset.from_tensor_slices(tokenized_datas_dict)\n",
    "tfdsmap_tokenized_data = tfds_tokenized_data.map(lambda x: {\n",
    "  'taken_courses': x['taken_courses'],\n",
    "  'recommendation': x['recommendation']\n",
    "})\n",
    "tfdsmap_tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "1719\n"
     ]
    }
   ],
   "source": [
    "print(len(tfdsmap_courses))\n",
    "print(len(tfdsmap_tokenized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec={'taken_courses': TensorSpec(shape=(5,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}> 1547\n",
      "<TakeDataset element_spec={'taken_courses': TensorSpec(shape=(5,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}> 172\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(69420)\n",
    "shuffled = tfdsmap_tokenized_data.shuffle(len(tfdsmap_tokenized_data), seed=69420, reshuffle_each_iteration=False)\n",
    "\n",
    "train_len = math.floor(len(tfdsmap_tokenized_data) * 0.9)\n",
    "test_len = len(tfdsmap_tokenized_data) - train_len\n",
    "\n",
    "train = shuffled.take(train_len)\n",
    "test = shuffled.skip(train_len).take(test_len)\n",
    "\n",
    "print(train, len(train))\n",
    "print(test, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32\n",
    "\n",
    "#https://www.tensorflow.org/recommenders/examples/multitask/\n",
    "\n",
    "user_properties_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(len(unique_course_names)+1, 32, input_length=5),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(embedding_dimension, activation='relu')\n",
    "])\n",
    "\n",
    "course_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(vocabulary=unique_course_names, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_course_names) + 1, embedding_dimension),\n",
    "  # tf.keras.layers.GRU(embedding_dimension)\n",
    "])\n",
    "\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=tfrs.metrics.FactorizedTopK(\n",
    "    candidates=tfdsmap_courses.batch(16).map(course_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.Model):\n",
    "  def __init__(self, user_model, course_model):\n",
    "    super().__init__()\n",
    "    self.course_model: tf.keras.Model = course_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[tf.Tensor, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"taken_courses\"])\n",
    "\n",
    "    # And pick out the course features and pass them into the course model,\n",
    "    # getting embeddings back.\n",
    "    positive_course_embeddings = self.course_model(features[\"recommendation\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_course_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 5, 32)             1568      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               16100     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                3232      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,900\n",
      "Trainable params: 20,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " string_lookup_7 (StringLook  (None,)                  0         \n",
      " up)                                                             \n",
      "                                                                 \n",
      " embedding_14 (Embedding)    (None, 32)                1568      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,568\n",
      "Trainable params: 1,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(user_properties_model, course_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "cached_train = train.shuffle(len(tfdsmap_tokenized_data)).batch(400).cache()\n",
    "cached_test = test.batch(40).cache()\n",
    "\n",
    "user_properties_model.summary()\n",
    "course_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 0s 74ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1008 - factorized_top_k/top_5_categorical_accuracy: 0.4202 - factorized_top_k/top_10_categorical_accuracy: 0.4544 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2196.9127 - regularization_loss: 0.0000e+00 - total_loss: 2196.9127\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0963 - factorized_top_k/top_5_categorical_accuracy: 0.4163 - factorized_top_k/top_10_categorical_accuracy: 0.4758 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2179.5165 - regularization_loss: 0.0000e+00 - total_loss: 2179.5165\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 69ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1209 - factorized_top_k/top_5_categorical_accuracy: 0.4085 - factorized_top_k/top_10_categorical_accuracy: 0.5016 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2168.6496 - regularization_loss: 0.0000e+00 - total_loss: 2168.6496\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1183 - factorized_top_k/top_5_categorical_accuracy: 0.4370 - factorized_top_k/top_10_categorical_accuracy: 0.5604 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2156.8757 - regularization_loss: 0.0000e+00 - total_loss: 2156.8757\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 63ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0666 - factorized_top_k/top_5_categorical_accuracy: 0.4467 - factorized_top_k/top_10_categorical_accuracy: 0.5572 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2169.9173 - regularization_loss: 0.0000e+00 - total_loss: 2169.9173\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 69ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0750 - factorized_top_k/top_5_categorical_accuracy: 0.4590 - factorized_top_k/top_10_categorical_accuracy: 0.5695 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2142.8908 - regularization_loss: 0.0000e+00 - total_loss: 2142.8908\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1144 - factorized_top_k/top_5_categorical_accuracy: 0.4460 - factorized_top_k/top_10_categorical_accuracy: 0.6141 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2113.9267 - regularization_loss: 0.0000e+00 - total_loss: 2113.9267\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 64ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1396 - factorized_top_k/top_5_categorical_accuracy: 0.4900 - factorized_top_k/top_10_categorical_accuracy: 0.6593 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2091.2456 - regularization_loss: 0.0000e+00 - total_loss: 2091.2456\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1474 - factorized_top_k/top_5_categorical_accuracy: 0.4822 - factorized_top_k/top_10_categorical_accuracy: 0.6787 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2067.7884 - regularization_loss: 0.0000e+00 - total_loss: 2067.7884\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 64ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1170 - factorized_top_k/top_5_categorical_accuracy: 0.4945 - factorized_top_k/top_10_categorical_accuracy: 0.6975 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2064.5517 - regularization_loss: 0.0000e+00 - total_loss: 2064.5517\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1054 - factorized_top_k/top_5_categorical_accuracy: 0.5346 - factorized_top_k/top_10_categorical_accuracy: 0.7292 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2037.1584 - regularization_loss: 0.0000e+00 - total_loss: 2037.1584\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 74ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1480 - factorized_top_k/top_5_categorical_accuracy: 0.5611 - factorized_top_k/top_10_categorical_accuracy: 0.7537 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2013.7434 - regularization_loss: 0.0000e+00 - total_loss: 2013.7434\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 71ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1474 - factorized_top_k/top_5_categorical_accuracy: 0.5688 - factorized_top_k/top_10_categorical_accuracy: 0.7673 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1999.9237 - regularization_loss: 0.0000e+00 - total_loss: 1999.9237\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 70ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1429 - factorized_top_k/top_5_categorical_accuracy: 0.5882 - factorized_top_k/top_10_categorical_accuracy: 0.7906 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1988.1569 - regularization_loss: 0.0000e+00 - total_loss: 1988.1569\n",
      "Epoch 15/20\n",
      "3/4 [=====================>........] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.1300 - factorized_top_k/top_5_categorical_accuracy: 0.6200 - factorized_top_k/top_10_categorical_accuracy: 0.7900 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2105.3713 - regularization_loss: 0.0000e+00 - total_loss: 2105.3713"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 14:34:42.643261: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 114ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1532 - factorized_top_k/top_5_categorical_accuracy: 0.6199 - factorized_top_k/top_10_categorical_accuracy: 0.7880 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1989.2483 - regularization_loss: 0.0000e+00 - total_loss: 1989.2483\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1648 - factorized_top_k/top_5_categorical_accuracy: 0.6458 - factorized_top_k/top_10_categorical_accuracy: 0.8061 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1948.0835 - regularization_loss: 0.0000e+00 - total_loss: 1948.0835\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 63ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1842 - factorized_top_k/top_5_categorical_accuracy: 0.6580 - factorized_top_k/top_10_categorical_accuracy: 0.8429 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1913.0863 - regularization_loss: 0.0000e+00 - total_loss: 1913.0863\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 64ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1377 - factorized_top_k/top_5_categorical_accuracy: 0.6729 - factorized_top_k/top_10_categorical_accuracy: 0.8494 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1901.8809 - regularization_loss: 0.0000e+00 - total_loss: 1901.8809\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1610 - factorized_top_k/top_5_categorical_accuracy: 0.6555 - factorized_top_k/top_10_categorical_accuracy: 0.8533 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1901.4947 - regularization_loss: 0.0000e+00 - total_loss: 1901.4947\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2036 - factorized_top_k/top_5_categorical_accuracy: 0.6761 - factorized_top_k/top_10_categorical_accuracy: 0.8668 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1884.7150 - regularization_loss: 0.0000e+00 - total_loss: 1884.7150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fab1044d760>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(cached_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 60ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0640 - factorized_top_k/top_5_categorical_accuracy: 0.5058 - factorized_top_k/top_10_categorical_accuracy: 0.6512 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 126.3178 - regularization_loss: 0.0000e+00 - total_loss: 126.3178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.06395348906517029,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.5058139562606812,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.6511628031730652,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 1.0,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 1.0,\n",
       " 'loss': 48.07685089111328,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 48.07685089111328}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7fab10619af0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "\n",
    "# recommends courses out of the entire courses dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((\n",
    "    tfdsmap_courses.batch(10), \n",
    "    tfdsmap_courses.batch(10).map(model.course_model)\n",
    "  ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 38 30 15  3]]\n",
      "Recommendation:\n",
      "Memulai Pemrograman Dengan Java\n",
      "Belajar Dasar UX Design\n",
      "Belajar Membuat Aplikasi Back-End untuk Pemula dengan Google Cloud\n",
      "Belajar Fundamental Aplikasi Back-End\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get recommendations.\n",
    "taken_courses = [\n",
    "  \"Belajar Dasar Pemrograman Web,Meniti Karier sebagai Software Developer\",\n",
    "  \"Belajar Membuat Augmented Reality dengan Lens Studio\",\n",
    "  \"Belajar Dasar Pemrograman JavaScript\",\n",
    "  \"Belajar Fundamental Front-End Web Development\",\n",
    "  \"Belajar Membuat Aplikasi Android untuk Pemula\"\n",
    "]\n",
    "inputdata = tokenize(tokenizer_dict, [taken_courses], MAX_HISTORY)\n",
    "print(inputdata)\n",
    "\n",
    "_, course_names = index(inputdata)\n",
    "print(\"Recommendation:\")\n",
    "for course_name in course_names[0, :4]:\n",
    "  tf.print(course_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
