{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 00:46:37.506973: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-10 00:46:37.509983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-10 00:46:37.509992: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import itertools\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "\n",
    "MAX_HISTORY = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load both dataset and convert to tfds object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slices(features):\n",
    "  for i in itertools.count():\n",
    "    # For each feature take index `i`\n",
    "    example = {name:values[i] for name, values in features.items()}\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load courses list\n",
    "TODO: Course list disini sepertinya rada galengkap, token dari course\" bangkit jadi nol semua\n",
    "\n",
    "Output berupa objek MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_name        : Belajar Fundamental Aplikasi Android\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 00:46:39.404591: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-10 00:46:39.404665: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-10 00:46:39.404707: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (yggdrasil): /proc/driver/nvidia/version does not exist\n",
      "2022-06-10 00:46:39.405550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = pd.read_csv(\"courses.csv\")\n",
    "unique_course_names = courses[\"name\"].tolist()\n",
    "\n",
    "courses_dict = {\n",
    "  \"course_name\": np.array(unique_course_names)\n",
    "}\n",
    "\n",
    "for example in slices(courses_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "\n",
    "tfds_courses = tf.data.Dataset.from_tensor_slices(courses_dict)\n",
    "tfdsmap_courses = tfds_courses.map(lambda x: x['course_name'])\n",
    "tfdsmap_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Belajar Fundamental Aplikasi Android': 1,\n",
       " 'Belajar Membangun LINE Chatbot': 2,\n",
       " 'Belajar Membuat Aplikasi Android untuk Pemula': 3,\n",
       " 'Memulai Pemrograman Dengan Java': 4,\n",
       " 'Memulai Pemrograman Dengan Kotlin': 5,\n",
       " 'Menjadi Azure Cloud Developer': 6,\n",
       " 'Memulai Pemrograman Dengan Python': 7,\n",
       " 'Memulai Pemrograman Dengan C': 8,\n",
       " 'Belajar Dasar Pemrograman Web': 9,\n",
       " 'Menjadi Google Cloud Engineer': 10,\n",
       " 'Belajar Dasar-Dasar Azure Cloud': 11,\n",
       " 'Memulai Pemrograman Dengan Swift': 12,\n",
       " 'Belajar Membangun LINE Front-end Framework (LIFF)': 13,\n",
       " 'Belajar Membuat Aplikasi Flutter untuk Pemula': 14,\n",
       " 'Belajar Fundamental Front-End Web Development': 15,\n",
       " 'Menjadi Android Developer Expert': 16,\n",
       " 'Belajar Prinsip Pemrograman SOLID': 17,\n",
       " 'Belajar Membuat Aplikasi iOS untuk Pemula': 18,\n",
       " 'Belajar Dasar Visualisasi Data': 19,\n",
       " 'Belajar Machine Learning untuk Pemula': 20,\n",
       " 'Belajar Pengembangan Machine Learning': 21,\n",
       " 'Memulai Pemrograman Dengan Dart': 22,\n",
       " 'Belajar Fundamental Aplikasi Flutter': 23,\n",
       " 'Menjadi Flutter Developer Expert': 24,\n",
       " 'Belajar Fundamental Aplikasi iOS': 25,\n",
       " 'Menjadi iOS Developer Expert': 26,\n",
       " 'Menjadi Front-End Web Developer Expert': 27,\n",
       " 'Memulai Dasar Pemrograman untuk Menjadi Pengembang Software': 28,\n",
       " 'Cloud Practitioner Essentials (Belajar Dasar AWS Cloud)': 29,\n",
       " 'Belajar Dasar Pemrograman JavaScript': 30,\n",
       " 'Belajar Membuat Aplikasi Back-End untuk Pemula': 31,\n",
       " 'Architecting on AWS (Membangun Arsitektur Cloud di AWS)': 32,\n",
       " 'Belajar Fundamental Aplikasi Back-End': 33,\n",
       " 'Menjadi Back-End Developer Expert': 34,\n",
       " 'Meniti Karier sebagai Software Developer': 35,\n",
       " 'Pengenalan Data pada Pemrograman (Data 101)': 36,\n",
       " 'Pengenalan ke Logika Pemrograman (Programming Logic 101)': 37,\n",
       " 'Belajar Membuat Augmented Reality dengan Lens Studio': 38,\n",
       " 'Belajar Membuat Front-End Web untuk Pemula': 39,\n",
       " 'Belajar Dasar Git dengan GitHub': 40,\n",
       " 'Machine Learning Terapan': 41,\n",
       " 'Evaluasi Penguasaan Machine Learning': 42,\n",
       " 'Menjadi Google Cloud Architect': 43,\n",
       " 'Belajar Dasar UX Design': 44,\n",
       " 'Belajar Dasar Google Cloud': 45,\n",
       " 'Belajar Membuat Aplikasi Back-End untuk Pemula dengan Google Cloud': 46,\n",
       " 'Belajar Pengembangan Aplikasi Android Intermediate': 47,\n",
       " 'Belajar Jaringan Komputer untuk Pemula': 48}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_tokenizer_dict(train_list):\n",
    "  return dict(zip(train_list, [i+1 for i in range(len(train_list))]))\n",
    "\n",
    "get_tokenizer_dict(unique_course_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken_courses      : ['Bangkit General Assessment', 'Belajar Dasar Pemrograman Web', 'Belajar Fundamental Aplikasi Android']\n",
      "recommendation     : Bangkit Android Assessment\n"
     ]
    }
   ],
   "source": [
    "# Load and clean data\n",
    "useract = pd.read_csv(\"user_activities.csv\")\n",
    "useract = useract.drop(columns=['id'])\n",
    "useract = useract.dropna(how='all')\n",
    "\n",
    "def mergeall(rowdata):\n",
    "    aggregated_datas = \",\".join([str(s) for s in list(rowdata.values) if not pd.isna(s)])\n",
    "    return aggregated_datas.split(\",\")\n",
    "\n",
    "def mergecourse(rowdata):\n",
    "    aggregated_datas = \",\".join([str(s) for s in [rowdata.graduated_courses, rowdata.on_progress_courses] if not pd.isna(s)])\n",
    "    return aggregated_datas.split(\",\")\n",
    "\n",
    "merged_datas = list(useract.apply(mergeall, axis = 1))\n",
    "merged_courses = list(useract.apply(mergecourse, axis = 1))\n",
    "\n",
    "agumented_datas_dict = {\n",
    "  'taken_courses': [],\n",
    "  'recommendation': []\n",
    "}\n",
    "\n",
    "# for each merged course, \n",
    "#   for each course in merged course,\n",
    "#     pick 1 as output, lainya jadi input\n",
    "#     push as new entry\n",
    "# TODO: \n",
    "#   This split is not too well-defined on merged courses with length longer than MAX_HISTORY.\n",
    "#   This is because after permutating possible input, trailing courses are cut off at token padding anyway.\n",
    "\n",
    "for i in range(len(merged_datas)):\n",
    "  merged_data = merged_datas[i]\n",
    "  merged_course = merged_courses[i]\n",
    "  \n",
    "  # Sliding window sebesar MAX_HISTORY + 1 untuk dimasukkan permutasi output\n",
    "  for j in range(max(len(merged_course)-MAX_HISTORY+2, 1)):\n",
    "    merged_course_window = merged_course[j: min(len(merged_course), MAX_HISTORY + j +1)]\n",
    "\n",
    "    # Permutasi data dalam window sebagai output. Untuk setiap n dalam S, Input: S-exclude-n, Output: n \n",
    "    for course in merged_course_window:\n",
    "      #Simpen dulu buat nanti kalo mau ditambahin ingfo non-courses\n",
    "      #agumented_datas_dict['x'].append([data for data in merged_data if data != course])\n",
    "      x = [data for data in merged_course_window if data != course]\n",
    "      y = course\n",
    "      agumented_datas_dict['taken_courses'].append(x)\n",
    "      agumented_datas_dict['recommendation'].append(y)\n",
    "\n",
    "for example in slices(agumented_datas_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load user activities list\n",
    "User Activities merujuk pada semua course yang pernah dan sedang diambil. Asumsinya adalah kalau banyak user yang ambil bebarengan, maka seharusnya course\" tsb berkaitan.\n",
    "\n",
    "Augmentasi data dilakukan sbb:\n",
    "```\n",
    "Jika course yang pernah diambil adalah [a, b, c, d, e]\n",
    "output akan diaugmentasi menjadi:\n",
    "x             | y\n",
    "--------------+----\n",
    "[a, b, c, d]  | e\n",
    "[a, b, c, e]  | d\n",
    "[a, b, d, e]  | c\n",
    "[a, c, d, e]  | b\n",
    "[b, c, d, e]  | a\n",
    "\n",
    "```\n",
    "\n",
    "Output berupa objek MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken_courses      : [0 0 0 0 0 9 1]\n",
      "recommendation     : Bangkit Android Assessment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec={'taken_courses': TensorSpec(shape=(7,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize manual karena usecasenya agak aneh\n",
    "\n",
    "def get_tokenizer_dict(train_list):\n",
    "  return dict(zip(train_list, [i+1 for i in range(len(train_list))]))\n",
    "\n",
    "def pad_tokens(input_sequences, maxlen):\n",
    "  return np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n",
    "\n",
    "def tokenize(tokenizer_dict, corpus, maxlen = 5):\n",
    "  output = []\n",
    "  for line in corpus:\n",
    "    tokenizedline = [tokenizer_dict.get(entry, 0) for entry in line]\n",
    "    output.append(tokenizedline)\n",
    "    \n",
    "  return pad_tokens(output, maxlen)\n",
    "\n",
    "\n",
    "tokenizer_dict = get_tokenizer_dict(unique_course_names)\n",
    "tokenized_datas = tokenize(tokenizer_dict, agumented_datas_dict[\"taken_courses\"], MAX_HISTORY)\n",
    "tokenized_datas_dict = {\n",
    "  'taken_courses': tokenized_datas,\n",
    "  'recommendation': agumented_datas_dict[\"recommendation\"]\n",
    "}\n",
    "\n",
    "for example in slices(tokenized_datas_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "\n",
    "tfds_tokenized_data = tf.data.Dataset.from_tensor_slices(tokenized_datas_dict)\n",
    "tfdsmap_tokenized_data = tfds_tokenized_data.map(lambda x: {\n",
    "  'taken_courses': x['taken_courses'],\n",
    "  'recommendation': x['recommendation']\n",
    "})\n",
    "tfdsmap_tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "3779\n"
     ]
    }
   ],
   "source": [
    "print(len(tfdsmap_courses))\n",
    "print(len(tfdsmap_tokenized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec={'taken_courses': TensorSpec(shape=(7,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}> 3401\n",
      "<TakeDataset element_spec={'taken_courses': TensorSpec(shape=(7,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}> 378\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42069)\n",
    "shuffled = tfdsmap_tokenized_data.shuffle(len(tfdsmap_tokenized_data), seed=42069, reshuffle_each_iteration=False)\n",
    "\n",
    "train_len = math.floor(len(tfdsmap_tokenized_data) * 0.9)\n",
    "test_len = len(tfdsmap_tokenized_data) - train_len\n",
    "\n",
    "train = shuffled.take(train_len)\n",
    "test = shuffled.skip(train_len).take(test_len)\n",
    "\n",
    "print(train, len(train))\n",
    "print(test, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32\n",
    "\n",
    "#https://www.tensorflow.org/recommenders/examples/multitask/\n",
    "\n",
    "user_properties_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(len(unique_course_names)+1, 32, input_length=MAX_HISTORY),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(150, activation='relu'),\n",
    "  tf.keras.layers.Dense(150, activation='relu'),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(embedding_dimension, activation='relu')\n",
    "])\n",
    "\n",
    "course_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(vocabulary=unique_course_names, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_course_names) + 1, embedding_dimension),\n",
    "])\n",
    "\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=tfrs.metrics.FactorizedTopK(\n",
    "    candidates=tfdsmap_courses.batch(16).map(course_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.Model):\n",
    "  def __init__(self, user_model, course_model):\n",
    "    super().__init__()\n",
    "    self.course_model: tf.keras.Model = course_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[tf.Tensor, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"taken_courses\"])\n",
    "\n",
    "    # And pick out the course features and pass them into the course model,\n",
    "    # getting embeddings back.\n",
    "    positive_course_embeddings = self.course_model(features[\"recommendation\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_course_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 7, 32)             1568      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               33750     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                3232      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,300\n",
      "Trainable params: 76,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " string_lookup (StringLookup  (None,)                  0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 32)                1568      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,568\n",
      "Trainable params: 1,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(user_properties_model, course_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "cached_train = train.shuffle(len(tfdsmap_tokenized_data)).batch(400).cache()\n",
    "cached_test = test.batch(40).cache()\n",
    "\n",
    "user_properties_model.summary()\n",
    "course_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "9/9 [==============================] - 2s 57ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0226 - factorized_top_k/top_5_categorical_accuracy: 0.1217 - factorized_top_k/top_10_categorical_accuracy: 0.2561 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2161.3125 - regularization_loss: 0.0000e+00 - total_loss: 2161.3125\n",
      "Epoch 2/40\n",
      "9/9 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0406 - factorized_top_k/top_5_categorical_accuracy: 0.1591 - factorized_top_k/top_10_categorical_accuracy: 0.2499 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2129.1721 - regularization_loss: 0.0000e+00 - total_loss: 2129.1721\n",
      "Epoch 3/40\n",
      "9/9 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0506 - factorized_top_k/top_5_categorical_accuracy: 0.1873 - factorized_top_k/top_10_categorical_accuracy: 0.2749 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2154.1573 - regularization_loss: 0.0000e+00 - total_loss: 2154.1573\n",
      "Epoch 4/40\n",
      "9/9 [==============================] - 0s 50ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0365 - factorized_top_k/top_5_categorical_accuracy: 0.2714 - factorized_top_k/top_10_categorical_accuracy: 0.3867 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2122.3622 - regularization_loss: 0.0000e+00 - total_loss: 2122.3622\n",
      "Epoch 5/40\n",
      "9/9 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0265 - factorized_top_k/top_5_categorical_accuracy: 0.1852 - factorized_top_k/top_10_categorical_accuracy: 0.3375 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2093.5801 - regularization_loss: 0.0000e+00 - total_loss: 2093.5801\n",
      "Epoch 6/40\n",
      "9/9 [==============================] - 1s 63ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0441 - factorized_top_k/top_5_categorical_accuracy: 0.2993 - factorized_top_k/top_10_categorical_accuracy: 0.3899 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2070.7608 - regularization_loss: 0.0000e+00 - total_loss: 2070.7608\n",
      "Epoch 7/40\n",
      "9/9 [==============================] - 1s 58ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0594 - factorized_top_k/top_5_categorical_accuracy: 0.3364 - factorized_top_k/top_10_categorical_accuracy: 0.4599 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2063.2614 - regularization_loss: 0.0000e+00 - total_loss: 2063.2614\n",
      "Epoch 8/40\n",
      "9/9 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1000 - factorized_top_k/top_5_categorical_accuracy: 0.3458 - factorized_top_k/top_10_categorical_accuracy: 0.4810 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2061.2505 - regularization_loss: 0.0000e+00 - total_loss: 2061.2505\n",
      "Epoch 9/40\n",
      "9/9 [==============================] - 0s 55ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0964 - factorized_top_k/top_5_categorical_accuracy: 0.3249 - factorized_top_k/top_10_categorical_accuracy: 0.4596 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2053.1229 - regularization_loss: 0.0000e+00 - total_loss: 2053.1229\n",
      "Epoch 10/40\n",
      "9/9 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0770 - factorized_top_k/top_5_categorical_accuracy: 0.3231 - factorized_top_k/top_10_categorical_accuracy: 0.4681 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2041.4013 - regularization_loss: 0.0000e+00 - total_loss: 2041.4013\n",
      "Epoch 11/40\n",
      "9/9 [==============================] - 0s 51ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0623 - factorized_top_k/top_5_categorical_accuracy: 0.3349 - factorized_top_k/top_10_categorical_accuracy: 0.4843 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2032.7624 - regularization_loss: 0.0000e+00 - total_loss: 2032.7624\n",
      "Epoch 12/40\n",
      "9/9 [==============================] - 0s 48ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0606 - factorized_top_k/top_5_categorical_accuracy: 0.3522 - factorized_top_k/top_10_categorical_accuracy: 0.5034 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2015.9282 - regularization_loss: 0.0000e+00 - total_loss: 2015.9282\n",
      "Epoch 13/40\n",
      "9/9 [==============================] - 1s 58ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0847 - factorized_top_k/top_5_categorical_accuracy: 0.3478 - factorized_top_k/top_10_categorical_accuracy: 0.5104 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2007.8938 - regularization_loss: 0.0000e+00 - total_loss: 2007.8938\n",
      "Epoch 14/40\n",
      "9/9 [==============================] - 0s 51ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0759 - factorized_top_k/top_5_categorical_accuracy: 0.3725 - factorized_top_k/top_10_categorical_accuracy: 0.5193 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2014.9264 - regularization_loss: 0.0000e+00 - total_loss: 2014.9264\n",
      "Epoch 15/40\n",
      "9/9 [==============================] - 0s 55ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0723 - factorized_top_k/top_5_categorical_accuracy: 0.3840 - factorized_top_k/top_10_categorical_accuracy: 0.5384 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1978.7491 - regularization_loss: 0.0000e+00 - total_loss: 1978.7491\n",
      "Epoch 16/40\n",
      "9/9 [==============================] - 1s 56ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0817 - factorized_top_k/top_5_categorical_accuracy: 0.3805 - factorized_top_k/top_10_categorical_accuracy: 0.5587 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1982.1614 - regularization_loss: 0.0000e+00 - total_loss: 1982.1614\n",
      "Epoch 17/40\n",
      "9/9 [==============================] - 1s 58ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0838 - factorized_top_k/top_5_categorical_accuracy: 0.3914 - factorized_top_k/top_10_categorical_accuracy: 0.5666 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1967.6881 - regularization_loss: 0.0000e+00 - total_loss: 1967.6881\n",
      "Epoch 18/40\n",
      "9/9 [==============================] - 1s 58ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1211 - factorized_top_k/top_5_categorical_accuracy: 0.4358 - factorized_top_k/top_10_categorical_accuracy: 0.6057 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1936.1719 - regularization_loss: 0.0000e+00 - total_loss: 1936.1719\n",
      "Epoch 19/40\n",
      "9/9 [==============================] - 1s 59ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1208 - factorized_top_k/top_5_categorical_accuracy: 0.4575 - factorized_top_k/top_10_categorical_accuracy: 0.6125 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1949.4731 - regularization_loss: 0.0000e+00 - total_loss: 1949.4731\n",
      "Epoch 20/40\n",
      "9/9 [==============================] - 0s 52ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1194 - factorized_top_k/top_5_categorical_accuracy: 0.4696 - factorized_top_k/top_10_categorical_accuracy: 0.6492 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1920.0686 - regularization_loss: 0.0000e+00 - total_loss: 1920.0686\n",
      "Epoch 21/40\n",
      "9/9 [==============================] - 1s 57ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1344 - factorized_top_k/top_5_categorical_accuracy: 0.5166 - factorized_top_k/top_10_categorical_accuracy: 0.6836 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1883.3348 - regularization_loss: 0.0000e+00 - total_loss: 1883.3348\n",
      "Epoch 22/40\n",
      "9/9 [==============================] - 0s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1441 - factorized_top_k/top_5_categorical_accuracy: 0.5454 - factorized_top_k/top_10_categorical_accuracy: 0.7168 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1857.6407 - regularization_loss: 0.0000e+00 - total_loss: 1857.6407\n",
      "Epoch 23/40\n",
      "9/9 [==============================] - 0s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1441 - factorized_top_k/top_5_categorical_accuracy: 0.5407 - factorized_top_k/top_10_categorical_accuracy: 0.7339 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1853.1313 - regularization_loss: 0.0000e+00 - total_loss: 1853.1313\n",
      "Epoch 24/40\n",
      "9/9 [==============================] - 0s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1382 - factorized_top_k/top_5_categorical_accuracy: 0.5263 - factorized_top_k/top_10_categorical_accuracy: 0.7177 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1863.7626 - regularization_loss: 0.0000e+00 - total_loss: 1863.7626\n",
      "Epoch 25/40\n",
      "9/9 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1941 - factorized_top_k/top_5_categorical_accuracy: 0.6113 - factorized_top_k/top_10_categorical_accuracy: 0.7845 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1768.7866 - regularization_loss: 0.0000e+00 - total_loss: 1768.7866\n",
      "Epoch 26/40\n",
      "9/9 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1888 - factorized_top_k/top_5_categorical_accuracy: 0.6119 - factorized_top_k/top_10_categorical_accuracy: 0.8021 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1773.7890 - regularization_loss: 0.0000e+00 - total_loss: 1773.7890\n",
      "Epoch 27/40\n",
      "9/9 [==============================] - 0s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1908 - factorized_top_k/top_5_categorical_accuracy: 0.6430 - factorized_top_k/top_10_categorical_accuracy: 0.8071 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1754.0443 - regularization_loss: 0.0000e+00 - total_loss: 1754.0443\n",
      "Epoch 28/40\n",
      "9/9 [==============================] - 1s 55ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2267 - factorized_top_k/top_5_categorical_accuracy: 0.6786 - factorized_top_k/top_10_categorical_accuracy: 0.8409 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1703.3784 - regularization_loss: 0.0000e+00 - total_loss: 1703.3784\n",
      "Epoch 29/40\n",
      "9/9 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2417 - factorized_top_k/top_5_categorical_accuracy: 0.7077 - factorized_top_k/top_10_categorical_accuracy: 0.8668 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1676.6413 - regularization_loss: 0.0000e+00 - total_loss: 1676.6413\n",
      "Epoch 30/40\n",
      "9/9 [==============================] - 0s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2405 - factorized_top_k/top_5_categorical_accuracy: 0.7113 - factorized_top_k/top_10_categorical_accuracy: 0.8550 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1690.0784 - regularization_loss: 0.0000e+00 - total_loss: 1690.0784\n",
      "Epoch 31/40\n",
      "9/9 [==============================] - 0s 49ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2273 - factorized_top_k/top_5_categorical_accuracy: 0.7298 - factorized_top_k/top_10_categorical_accuracy: 0.8815 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1652.7558 - regularization_loss: 0.0000e+00 - total_loss: 1652.7558\n",
      "Epoch 32/40\n",
      "9/9 [==============================] - 1s 55ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2279 - factorized_top_k/top_5_categorical_accuracy: 0.7254 - factorized_top_k/top_10_categorical_accuracy: 0.8700 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1652.1402 - regularization_loss: 0.0000e+00 - total_loss: 1652.1402\n",
      "Epoch 33/40\n",
      "9/9 [==============================] - 1s 56ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2685 - factorized_top_k/top_5_categorical_accuracy: 0.7542 - factorized_top_k/top_10_categorical_accuracy: 0.8950 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1607.8120 - regularization_loss: 0.0000e+00 - total_loss: 1607.8120\n",
      "Epoch 34/40\n",
      "9/9 [==============================] - 1s 57ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2685 - factorized_top_k/top_5_categorical_accuracy: 0.7765 - factorized_top_k/top_10_categorical_accuracy: 0.9047 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1582.6269 - regularization_loss: 0.0000e+00 - total_loss: 1582.6269\n",
      "Epoch 35/40\n",
      "9/9 [==============================] - 1s 55ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2852 - factorized_top_k/top_5_categorical_accuracy: 0.7995 - factorized_top_k/top_10_categorical_accuracy: 0.9191 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1550.6040 - regularization_loss: 0.0000e+00 - total_loss: 1550.6040\n",
      "Epoch 36/40\n",
      "9/9 [==============================] - 0s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2773 - factorized_top_k/top_5_categorical_accuracy: 0.8230 - factorized_top_k/top_10_categorical_accuracy: 0.9324 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1507.0859 - regularization_loss: 0.0000e+00 - total_loss: 1507.0859\n",
      "Epoch 37/40\n",
      "9/9 [==============================] - 1s 55ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2905 - factorized_top_k/top_5_categorical_accuracy: 0.8362 - factorized_top_k/top_10_categorical_accuracy: 0.9383 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1479.4377 - regularization_loss: 0.0000e+00 - total_loss: 1479.4377\n",
      "Epoch 38/40\n",
      "9/9 [==============================] - 1s 56ms/step - factorized_top_k/top_1_categorical_accuracy: 0.3049 - factorized_top_k/top_5_categorical_accuracy: 0.8445 - factorized_top_k/top_10_categorical_accuracy: 0.9453 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1487.9125 - regularization_loss: 0.0000e+00 - total_loss: 1487.9125\n",
      "Epoch 39/40\n",
      "9/9 [==============================] - 1s 55ms/step - factorized_top_k/top_1_categorical_accuracy: 0.3128 - factorized_top_k/top_5_categorical_accuracy: 0.8421 - factorized_top_k/top_10_categorical_accuracy: 0.9368 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1480.9161 - regularization_loss: 0.0000e+00 - total_loss: 1480.9161\n",
      "Epoch 40/40\n",
      "7/9 [======================>.......] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.3432 - factorized_top_k/top_5_categorical_accuracy: 0.8679 - factorized_top_k/top_10_categorical_accuracy: 0.9479 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1629.2392 - regularization_loss: 0.0000e+00 - total_loss: 1629.2392"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 00:47:01.603833: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 63ms/step - factorized_top_k/top_1_categorical_accuracy: 0.3484 - factorized_top_k/top_5_categorical_accuracy: 0.8703 - factorized_top_k/top_10_categorical_accuracy: 0.9485 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1433.3435 - regularization_loss: 0.0000e+00 - total_loss: 1433.3435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f586378e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 39ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2381 - factorized_top_k/top_5_categorical_accuracy: 0.5423 - factorized_top_k/top_10_categorical_accuracy: 0.6825 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 177.9756 - regularization_loss: 0.0000e+00 - total_loss: 177.9756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.2380952388048172,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.5423280596733093,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.682539701461792,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 1.0,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 1.0,\n",
       " 'loss': 116.98431396484375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 116.98431396484375}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7f5f5827c400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "\n",
    "# recommends courses out of the entire courses dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((\n",
    "    tfdsmap_courses.batch(10), \n",
    "    tfdsmap_courses.batch(10).map(model.course_model)\n",
    "  ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]]\n",
      "Recommendation:\n",
      "Belajar Membuat Aplikasi Android untuk Pemula\n",
      "Memulai Pemrograman Dengan Java\n",
      "Belajar Membangun LINE Chatbot\n",
      "Belajar Fundamental Aplikasi Android\n",
      "Menjadi Azure Cloud Developer\n"
     ]
    }
   ],
   "source": [
    "taken_courses = []\n",
    "inputdata = tokenize(tokenizer_dict, [taken_courses], MAX_HISTORY)\n",
    "print(inputdata)\n",
    "_, course_names = index(inputdata)\n",
    "print(\"Recommendation:\")\n",
    "for course_name in course_names[0, :5]:\n",
    "  tf.print(course_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29 35  9 32 40 15  3]]\n",
      "Recommendation:\n",
      "Belajar Dasar Pemrograman JavaScript\n",
      "Architecting on AWS (Membangun Arsitektur Cloud di AWS)\n",
      "Belajar Membuat Aplikasi Back-End untuk Pemula\n",
      "Belajar Dasar Git dengan GitHub\n",
      "Belajar Membuat Front-End Web untuk Pemula\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get recommendations.\n",
    "taken_courses = [\n",
    "  \"Cloud Practitioner Essentials (Belajar Dasar AWS Cloud)\",\n",
    "  \"Meniti Karier sebagai Software Developer\",\n",
    "  \"Belajar Dasar Pemrograman Web\",\n",
    "  \"Architecting on AWS (Membangun Arsitektur Cloud di AWS)\",\n",
    "  \"Belajar Dasar Git dengan GitHub\",\n",
    "  \"Belajar Fundamental Front-End Web Development\",\n",
    "  \"Belajar Membuat Aplikasi Android untuk Pemula\"\n",
    "]\n",
    "inputdata = tokenize(tokenizer_dict, [taken_courses], MAX_HISTORY)\n",
    "print(inputdata)\n",
    "\n",
    "_, course_names = index(inputdata)\n",
    "print(\"Recommendation:\")\n",
    "for course_name in course_names[0, :5]:\n",
    "  tf.print(course_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_index/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_index/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(index, \"./saved_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation:\n",
      "Belajar Dasar Pemrograman JavaScript\n",
      "Architecting on AWS (Membangun Arsitektur Cloud di AWS)\n",
      "Belajar Membuat Aplikasi Back-End untuk Pemula\n",
      "Belajar Dasar Git dengan GitHub\n"
     ]
    }
   ],
   "source": [
    "# load it back; can also be done in TensorFlow Serving.\n",
    "loaded = tf.saved_model.load(\"./saved_index\")\n",
    "\n",
    "# pass a user id in, get top predicted movie titles back.\n",
    "scores, titles = loaded(inputdata)\n",
    "\n",
    "_, course_names = loaded(inputdata)\n",
    "print(\"Recommendation:\")\n",
    "for course_name in course_names[0, :5]:\n",
    "  tf.print(course_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
