{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 19:00:13.271942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 19:00:13.271992: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import itertools\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "\n",
    "MAX_HISTORY = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load both dataset and convert to tfds object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slices(features):\n",
    "  for i in itertools.count():\n",
    "    # For each feature take index `i`\n",
    "    example = {name:values[i] for name, values in features.items()}\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load courses list\n",
    "TODO: Course list disini sepertinya rada galengkap, token dari course\" bangkit jadi nol semua\n",
    "\n",
    "Output berupa objek MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_name        : Belajar Fundamental Aplikasi Android\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 19:01:16.333706: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-07 19:01:16.333784: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-07 19:01:16.333842: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ratatoskr): /proc/driver/nvidia/version does not exist\n",
      "2022-06-07 19:01:16.539292: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = pd.read_csv(\"courses.csv\")\n",
    "unique_course_names = courses[\"name\"].tolist()\n",
    "\n",
    "courses_dict = {\n",
    "  \"course_name\": np.array(unique_course_names)\n",
    "}\n",
    "\n",
    "for example in slices(courses_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "\n",
    "tfds_courses = tf.data.Dataset.from_tensor_slices(courses_dict)\n",
    "tfdsmap_courses = tfds_courses.map(lambda x: x['course_name'])\n",
    "tfdsmap_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Belajar Fundamental Aplikasi Android': 1,\n",
       " 'Belajar Membangun LINE Chatbot': 2,\n",
       " 'Belajar Membuat Aplikasi Android untuk Pemula': 3,\n",
       " 'Memulai Pemrograman Dengan Java': 4,\n",
       " 'Memulai Pemrograman Dengan Kotlin': 5,\n",
       " 'Menjadi Azure Cloud Developer': 6,\n",
       " 'Memulai Pemrograman Dengan Python': 7,\n",
       " 'Memulai Pemrograman Dengan C': 8,\n",
       " 'Belajar Dasar Pemrograman Web': 9,\n",
       " 'Menjadi Google Cloud Engineer': 10,\n",
       " 'Belajar Dasar-Dasar Azure Cloud': 11,\n",
       " 'Memulai Pemrograman Dengan Swift': 12,\n",
       " 'Belajar Membangun LINE Front-end Framework (LIFF)': 13,\n",
       " 'Belajar Membuat Aplikasi Flutter untuk Pemula': 14,\n",
       " 'Belajar Fundamental Front-End Web Development': 15,\n",
       " 'Menjadi Android Developer Expert': 16,\n",
       " 'Belajar Prinsip Pemrograman SOLID': 17,\n",
       " 'Belajar Membuat Aplikasi iOS untuk Pemula': 18,\n",
       " 'Belajar Dasar Visualisasi Data': 19,\n",
       " 'Belajar Machine Learning untuk Pemula': 20,\n",
       " 'Belajar Pengembangan Machine Learning': 21,\n",
       " 'Memulai Pemrograman Dengan Dart': 22,\n",
       " 'Belajar Fundamental Aplikasi Flutter': 23,\n",
       " 'Menjadi Flutter Developer Expert': 24,\n",
       " 'Belajar Fundamental Aplikasi iOS': 25,\n",
       " 'Menjadi iOS Developer Expert': 26,\n",
       " 'Menjadi Front-End Web Developer Expert': 27,\n",
       " 'Memulai Dasar Pemrograman untuk Menjadi Pengembang Software': 28,\n",
       " 'Cloud Practitioner Essentials (Belajar Dasar AWS Cloud)': 29,\n",
       " 'Belajar Dasar Pemrograman JavaScript': 30,\n",
       " 'Belajar Membuat Aplikasi Back-End untuk Pemula': 31,\n",
       " 'Architecting on AWS (Membangun Arsitektur Cloud di AWS)': 32,\n",
       " 'Belajar Fundamental Aplikasi Back-End': 33,\n",
       " 'Menjadi Back-End Developer Expert': 34,\n",
       " 'Meniti Karier sebagai Software Developer': 35,\n",
       " 'Pengenalan Data pada Pemrograman (Data 101)': 36,\n",
       " 'Pengenalan ke Logika Pemrograman (Programming Logic 101)': 37,\n",
       " 'Belajar Membuat Augmented Reality dengan Lens Studio': 38,\n",
       " 'Belajar Membuat Front-End Web untuk Pemula': 39,\n",
       " 'Belajar Dasar Git dengan GitHub': 40,\n",
       " 'Machine Learning Terapan': 41,\n",
       " 'Evaluasi Penguasaan Machine Learning': 42,\n",
       " 'Menjadi Google Cloud Architect': 43,\n",
       " 'Belajar Dasar UX Design': 44,\n",
       " 'Belajar Dasar Google Cloud': 45,\n",
       " 'Belajar Membuat Aplikasi Back-End untuk Pemula dengan Google Cloud': 46,\n",
       " 'Belajar Pengembangan Aplikasi Android Intermediate': 47,\n",
       " 'Belajar Jaringan Komputer untuk Pemula': 48}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_tokenizer_dict(train_list):\n",
    "  return dict(zip(train_list, [i+1 for i in range(len(train_list))]))\n",
    "\n",
    "get_tokenizer_dict(unique_course_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken_courses      : ['Bangkit General Assessment', 'Belajar Dasar Pemrograman Web', 'Belajar Fundamental Aplikasi Android']\n",
      "recommendation     : Bangkit Android Assessment\n"
     ]
    }
   ],
   "source": [
    "# Load and clean data\n",
    "useract = pd.read_csv(\"user_activities.csv\")\n",
    "useract = useract.drop(columns=['id'])\n",
    "useract = useract.dropna(how='all')\n",
    "\n",
    "def mergeall(rowdata):\n",
    "    aggregated_datas = \",\".join([str(s) for s in list(rowdata.values) if not pd.isna(s)])\n",
    "    return aggregated_datas.split(\",\")\n",
    "\n",
    "def mergecourse(rowdata):\n",
    "    aggregated_datas = \",\".join([str(s) for s in [rowdata.graduated_courses, rowdata.on_progress_courses] if not pd.isna(s)])\n",
    "    return aggregated_datas.split(\",\")\n",
    "\n",
    "merged_datas = list(useract.apply(mergeall, axis = 1))\n",
    "merged_courses = list(useract.apply(mergecourse, axis = 1))\n",
    "\n",
    "agumented_datas_dict = {\n",
    "  'taken_courses': [],\n",
    "  'recommendation': []\n",
    "}\n",
    "\n",
    "# for each merged course, \n",
    "#   for each course in merged course,\n",
    "#     pick 1 as output, lainya jadi input\n",
    "#     push as new entry\n",
    "# TODO: \n",
    "#   This split is not too well-defined on merged courses with length longer than MAX_HISTORY.\n",
    "#   This is because after permutating possible input, trailing courses are cut off at token padding anyway.\n",
    "\n",
    "for i in range(len(merged_datas)):\n",
    "  merged_data = merged_datas[i]\n",
    "  merged_course = merged_courses[i]\n",
    "    \n",
    "  for course in merged_course:\n",
    "    #Simpen dulu buat nanti kalo mau ditambahin ingfo non-courses\n",
    "    #agumented_datas_dict['x'].append([data for data in merged_data if data != course])\n",
    "    agumented_datas_dict['taken_courses'].append([data for data in merged_course if data != course])\n",
    "    agumented_datas_dict['recommendation'].append(course)\n",
    "\n",
    "for example in slices(agumented_datas_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load user activities list\n",
    "User Activities merujuk pada semua course yang pernah dan sedang diambil. Asumsinya adalah kalau banyak user yang ambil bebarengan, maka seharusnya course\" tsb berkaitan.\n",
    "\n",
    "Augmentasi data dilakukan sbb:\n",
    "```\n",
    "Jika course yang pernah diambil adalah [a, b, c, d, e]\n",
    "output akan diaugmentasi menjadi:\n",
    "x             | y\n",
    "--------------+----\n",
    "[a, b, c, d]  | e\n",
    "[a, b, c, e]  | d\n",
    "[a, b, d, e]  | c\n",
    "[a, c, d, e]  | b\n",
    "[b, c, d, e]  | a\n",
    "\n",
    "```\n",
    "\n",
    "Output berupa objek MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken_courses      : [0 0 0 0 0 9 1]\n",
      "recommendation     : Bangkit Android Assessment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec={'taken_courses': TensorSpec(shape=(7,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize manual karena usecasenya agak aneh\n",
    "\n",
    "def get_tokenizer_dict(train_list):\n",
    "  return dict(zip(train_list, [i+1 for i in range(len(train_list))]))\n",
    "\n",
    "def pad_tokens(input_sequences, maxlen):\n",
    "  return np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n",
    "\n",
    "def tokenize(tokenizer_dict, corpus, maxlen = 5):\n",
    "  output = []\n",
    "  for line in corpus:\n",
    "    tokenizedline = [tokenizer_dict.get(entry, 0) for entry in line]\n",
    "    output.append(tokenizedline)\n",
    "    \n",
    "  return pad_tokens(output, maxlen)\n",
    "\n",
    "\n",
    "tokenizer_dict = get_tokenizer_dict(unique_course_names)\n",
    "tokenized_datas = tokenize(tokenizer_dict, agumented_datas_dict[\"taken_courses\"], MAX_HISTORY)\n",
    "tokenized_datas_dict = {\n",
    "  'taken_courses': tokenized_datas,\n",
    "  'recommendation': agumented_datas_dict[\"recommendation\"]\n",
    "}\n",
    "\n",
    "for example in slices(tokenized_datas_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "\n",
    "tfds_tokenized_data = tf.data.Dataset.from_tensor_slices(tokenized_datas_dict)\n",
    "tfdsmap_tokenized_data = tfds_tokenized_data.map(lambda x: {\n",
    "  'taken_courses': x['taken_courses'],\n",
    "  'recommendation': x['recommendation']\n",
    "})\n",
    "tfdsmap_tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "1719\n"
     ]
    }
   ],
   "source": [
    "print(len(tfdsmap_courses))\n",
    "print(len(tfdsmap_tokenized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec={'taken_courses': TensorSpec(shape=(7,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}> 1547\n",
      "<TakeDataset element_spec={'taken_courses': TensorSpec(shape=(7,), dtype=tf.int32, name=None), 'recommendation': TensorSpec(shape=(), dtype=tf.string, name=None)}> 172\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42069)\n",
    "shuffled = tfdsmap_tokenized_data.shuffle(len(tfdsmap_tokenized_data), seed=69420, reshuffle_each_iteration=False)\n",
    "\n",
    "train_len = math.floor(len(tfdsmap_tokenized_data) * 0.9)\n",
    "test_len = len(tfdsmap_tokenized_data) - train_len\n",
    "\n",
    "train = shuffled.take(train_len)\n",
    "test = shuffled.skip(train_len).take(test_len)\n",
    "\n",
    "print(train, len(train))\n",
    "print(test, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32\n",
    "\n",
    "#https://www.tensorflow.org/recommenders/examples/multitask/\n",
    "\n",
    "user_properties_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(len(unique_course_names)+1, 32, input_length=MAX_HISTORY),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(150, activation='relu'),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(embedding_dimension, activation='relu')\n",
    "])\n",
    "\n",
    "course_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(vocabulary=unique_course_names, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_course_names) + 1, embedding_dimension),\n",
    "])\n",
    "\n",
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=tfrs.metrics.FactorizedTopK(\n",
    "    candidates=tfdsmap_courses.batch(16).map(course_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.Model):\n",
    "  def __init__(self, user_model, course_model):\n",
    "    super().__init__()\n",
    "    self.course_model: tf.keras.Model = course_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[tf.Tensor, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"taken_courses\"])\n",
    "\n",
    "    # And pick out the course features and pass them into the course model,\n",
    "    # getting embeddings back.\n",
    "    positive_course_embeddings = self.course_model(features[\"recommendation\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_course_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 7, 32)             1568      \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 224)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 150)               33750     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                3232      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,650\n",
      "Trainable params: 53,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " string_lookup_5 (StringLook  (None,)                  0         \n",
      " up)                                                             \n",
      "                                                                 \n",
      " embedding_11 (Embedding)    (None, 32)                1568      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,568\n",
      "Trainable params: 1,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(user_properties_model, course_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
    "\n",
    "cached_train = train.shuffle(len(tfdsmap_tokenized_data)).batch(400).cache()\n",
    "cached_test = test.batch(40).cache()\n",
    "\n",
    "user_properties_model.summary()\n",
    "course_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 2s 68ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0129 - factorized_top_k/top_5_categorical_accuracy: 0.1067 - factorized_top_k/top_10_categorical_accuracy: 0.1810 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2973.8888 - regularization_loss: 0.0000e+00 - total_loss: 2973.8888\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0129 - factorized_top_k/top_5_categorical_accuracy: 0.2043 - factorized_top_k/top_10_categorical_accuracy: 0.2954 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2429.0603 - regularization_loss: 0.0000e+00 - total_loss: 2429.0603\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2967 - factorized_top_k/top_5_categorical_accuracy: 0.4163 - factorized_top_k/top_10_categorical_accuracy: 0.4686 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2299.1049 - regularization_loss: 0.0000e+00 - total_loss: 2299.1049\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 78ms/step - factorized_top_k/top_1_categorical_accuracy: 0.7944 - factorized_top_k/top_5_categorical_accuracy: 0.8125 - factorized_top_k/top_10_categorical_accuracy: 0.8649 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2259.7046 - regularization_loss: 0.0000e+00 - total_loss: 2259.7046\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.9741 - factorized_top_k/top_5_categorical_accuracy: 0.9780 - factorized_top_k/top_10_categorical_accuracy: 0.9806 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2253.5396 - regularization_loss: 0.0000e+00 - total_loss: 2253.5396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9324326b50>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.9884 - factorized_top_k/top_5_categorical_accuracy: 0.9884 - factorized_top_k/top_10_categorical_accuracy: 0.9942 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 109.5345 - regularization_loss: 0.0000e+00 - total_loss: 109.5345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.9883720874786377,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.9883720874786377,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.9941860437393188,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 1.0,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 1.0,\n",
       " 'loss': 29.818880081176758,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 29.818880081176758}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7f9345e45250>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "\n",
    "# recommends courses out of the entire courses dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((\n",
    "    tfdsmap_courses.batch(10), \n",
    "    tfdsmap_courses.batch(10).map(model.course_model)\n",
    "  ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
       "array([[b'Belajar Fundamental Aplikasi Android',\n",
       "        b'Belajar Membangun LINE Chatbot',\n",
       "        b'Belajar Membuat Aplikasi Android untuk Pemula',\n",
       "        b'Memulai Pemrograman Dengan Java',\n",
       "        b'Memulai Pemrograman Dengan Kotlin',\n",
       "        b'Menjadi Azure Cloud Developer',\n",
       "        b'Memulai Pemrograman Dengan Python',\n",
       "        b'Memulai Pemrograman Dengan C',\n",
       "        b'Belajar Dasar Pemrograman Web',\n",
       "        b'Menjadi Google Cloud Engineer']], dtype=object)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taken_courses = [\n",
    "\n",
    "]\n",
    "inputdata = tokenize(tokenizer_dict, [taken_courses], MAX_HISTORY)\n",
    "print(inputdata)\n",
    "_, course_names = index(inputdata)\n",
    "course_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 38 30 15  3]]\n",
      "Recommendation:\n",
      "Belajar Fundamental Aplikasi Android\n",
      "Belajar Membangun LINE Chatbot\n",
      "Belajar Membuat Aplikasi Android untuk Pemula\n",
      "Memulai Pemrograman Dengan Java\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get recommendations.\n",
    "taken_courses = [\n",
    "  \"Belajar Dasar Pemrograman Web,Meniti Karier sebagai Software Developer\",\n",
    "  \"Belajar Membuat Augmented Reality dengan Lens Studio\",\n",
    "  \"Belajar Dasar Pemrograman JavaScript\",\n",
    "  \"Belajar Fundamental Front-End Web Development\",\n",
    "  \"Belajar Membuat Aplikasi Android untuk Pemula\"\n",
    "]\n",
    "inputdata = tokenize(tokenizer_dict, [taken_courses], MAX_HISTORY)\n",
    "print(inputdata)\n",
    "\n",
    "_, course_names = index(inputdata)\n",
    "print(\"Recommendation:\")\n",
    "for course_name in course_names[0, :4]:\n",
    "  tf.print(course_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_index/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_index/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(index, \"./saved_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation:\n",
      "Belajar Fundamental Aplikasi Android\n",
      "Belajar Membangun LINE Chatbot\n",
      "Belajar Membuat Aplikasi Android untuk Pemula\n",
      "Memulai Pemrograman Dengan Java\n"
     ]
    }
   ],
   "source": [
    "# load it back; can also be done in TensorFlow Serving.\n",
    "loaded = tf.saved_model.load(\"./saved_index\")\n",
    "\n",
    "# pass a user id in, get top predicted movie titles back.\n",
    "scores, titles = loaded(inputdata)\n",
    "\n",
    "_, course_names = loaded(inputdata)\n",
    "print(\"Recommendation:\")\n",
    "for course_name in course_names[0, :4]:\n",
    "  tf.print(course_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
